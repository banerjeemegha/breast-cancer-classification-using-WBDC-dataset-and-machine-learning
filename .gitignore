#import nesessary libraries and dataset
mport pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
data = pd.read_csv('../input/data.csv')
del data['Unnamed: 32']
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D
from keras.utils import to_categorical
from keras.callbacks import ModelCheckpoint
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.utils import class_weight

#classifier
classifier = Sequential()
y = data.iloc[:, 1].values
from sklearn.preprocessing import LabelEncoder
labelencoder_X_1 = LabelEncoder()
y = labelencoder_X_1.fit_transform(y)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state= 0)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
classifier.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=30))
classifier.add(Dropout(p=0.1))
classifier.add(Dense(output_dim=16, init='uniform', activation='relu'))
classifier.add(Dropout(p=0.1))
classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))
classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
classifier.summary()

#classifier training and testing
class_weights = class_weight.compute_class_weight('balanced',
np.unique(y_train),y_train)
history = classifier.fit(X_train, y_train, batch_size=300, nb_epoch=10, validation_data=(X_test, y_test),class_weight=class_weights,shuffle=True)
classifier.fit(X_train, y_train)
print("Accuracy of the model on Training Data is - " , classifier.evaluate(X_train,y_train)[1]*100)
print("Accuracy of the model on Testing Data is - " , classifier.evaluate(X_test,y_test)[1]*100)

#classification report, RoC curve, Confusion matrix
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)
from sklearn.metrics import confusion_matrix
cm = print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred, target_names =['Benign','Malignant']))
cm = pd.DataFrame(cm , index = ['Benign','Malignant'] , columns = ['Benign','Malignant'])
n_classes = 2
c_names = ['Benign', 'Malignant']
pred_prob = classifier.predict_proba(X_test)
fpr, tpr, thresh = roc_curve(y_test, pred_prob, pos_label=1)
random_probs = [0 for i in range(len(y_test))]
p_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)
from sklearn.metrics import roc_auc_score
auc_score = roc_auc_score(y_test, pred_prob)
print(auc_score)

#RoC curve
import matplotlib.pyplot as plt
plt.style.use('seaborn')
plt.plot(fpr, tpr, linestyle='-',color='red', label='ROC curve area')
plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')
plt.title('ROC curve', fontsize= 18)
plt.xlabel('False Positive Rate', fontsize= 15)
plt.ylabel('True Positive rate', fontsize= 15)
plt.legend(loc='best', fontsize= 13)
plt.savefig('ROC',dpi=300)
plt.show();
plt.savefig('xyz.eps', dpi=300)
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
fpr, tpr, _ = roc_curve(y_test, preds)
roc_auc = auc(fpr, tpr)
In [144]:

# Plot ROC curves
fig, ax = plt.subplots(figsize=(14, 8))
ax.plot([0, 1], [0, 1], 'k--')
ax.set_xlim([0.0, 1.0])
ax.set_ylim([0.0, 1.05])
ax.set_xlabel('False Positive Rate', fontsize=22)
plt.rc('xtick', labelsize=17)
ax.set_ylabel('True Positive Rate', fontsize=22)
plt.rc('ytick', labelsize=17)
ax.set_title('ROC Curve for Each Class', fontsize=22)
for i in range(n_classes):
ax.plot(fpr, tpr, linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc, c_names))
ax.legend(loc="best", fontsize='20')
ax.grid(alpha=.4)
sns.despine()
plt.show()
plt.figure(figsize = [20,25])
plt.figure()
plt.plot(fpr, tpr, color='red', label='ROC curve (area = %0.2f)' % roc_auc)
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=15)
plt.ylabel('True Positive Rate', fontsize=15)
plt.title('Receiver operating characteristic', fontsize=16)
plt.legend(loc="lower right", fontsize=13)
plt.show()

#accuracy and loss curve
plt.figure(figsize = (18,6))
plt.subplot(2,1,1)
plt.title('Accuracy Curve', fontsize=19)
plt.plot(history.history['acc'], label = 'training acc')
plt.plot(history.history['val_acc'], label = 'validation acc')
plt.rc('xtick', labelsize=16)
plt.rc('ytick', labelsize=16)
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Accuracy',fontsize=16)
plt.legend(fontsize=12)
plt.subplot(2,1,2)
plt.plot(history.history['loss'], label = 'training loss')
plt.plot(history.history['val_loss'], label = 'validation loss')
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Loss',fontsize=16)
plt.legend(fontsize=12)
plt.rc('xtick', labelsize=15)
plt.rc('ytick', labelsize=15)
plt.title('Loss Curve', fontsize=19)
epochs = [i for i in range(10)]
fig , ax = plt.subplots(1,2)
train_acc = history.history['acc']
train_loss = history.history['loss']
val_acc = history.history['val_acc']
val_loss = history.history['val_loss']
fig.set_size_inches(20,10)
ax[0].plot(epochs , train_acc , 'go-' , color='b', label = 'Training Accuracy')
ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')
ax[0].set_title('Accuracy Curve', fontsize='16')
ax[0].legend()
ax[0].set_xlabel("Epochs", fontsize='14')
ax[0].set_ylabel("Accuracy", fontsize='14')
ax[1].plot(epochs , train_loss , 'go-' ,color='b', label = 'Training Loss')
ax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')
ax[1].set_title('Loss Curve', fontsize='16')
ax[1].legend()
ax[1].set_xlabel("Epochs", fontsize='14')
ax[1].set_ylabel("Loss", fontsize='14')
plt.show()
